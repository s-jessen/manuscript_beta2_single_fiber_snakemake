Building DAG of jobs...
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job          count
---------  -------
data_prep        1
total            1

Select jobs to execute...

[Mon Aug  4 10:08:59 2025]
rule data_prep:
    input: data-raw/proteomic_data.csv, data-raw/design.xlsx, data-raw/keywords.xlsx, data-raw/mitocarta.xls
    output: data/metadata.rda, data/se_raw.rda, data/se.rda, data/df_long.rda, data/df_long_l2fc.rda, data/df_long_l2fc_mean.rda
    jobid: 0
    reason: Forced execution
    resources: tmpdir=C:\Users\SRENJE~1\AppData\Local\Temp

[Mon Aug  4 10:08:59 2025]
Error in rule data_prep:
    jobid: 0
    input: data-raw/proteomic_data.csv, data-raw/design.xlsx, data-raw/keywords.xlsx, data-raw/mitocarta.xls
    output: data/metadata.rda, data/se_raw.rda, data/se.rda, data/df_long.rda, data/df_long_l2fc.rda, data/df_long_l2fc_mean.rda

RuleException:
WorkflowError in file C:\R\manuscript_beta2_single_fiber_snakemake\Snakefile, line 17:
Failed to open source file C:\R\manuscript_beta2_single_fiber_snakemake\R/data_prep.R
FileNotFoundError: [Errno 2] No such file or directory: 'C:\\R\\manuscript_beta2_single_fiber_snakemake\\R/data_prep.R'
  File "C:\R\manuscript_beta2_single_fiber_snakemake\Snakefile", line 17, in __rule_data_prep
  File "C:\miniconda3\envs\snakemake_env\Lib\concurrent\futures\thread.py", line 58, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake\log\2025-08-04T100859.215778.snakemake.log
